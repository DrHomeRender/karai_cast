📁 util 폴더 구성 설명
이 폴더는 Transformer 모델의 구성요소들을 분리해놓은 곳입니다.
쉽게 말하면, 큰 Transformer 모델을 만들기 위해 필요한 부품들이 들어있습니다.
각 파일은 하나의 역할을 담당하고 있고, 서로 조립되어서 최종 모델이 만들어집니다.
https://newsight.tistory.com/330
🔧 Encoder.py
Transformer의 인코더 레이어 하나를 정의한 파일입니다.

Multi-Head Attention → FFN (피드포워드 신경망) → 출력

실제로 데이터를 분석하고 정보를 압축해주는 부분입니다.

모델에서 여러 개를 쌓아 사용하는 구조로 되어 있음.

🔧 Decoder.py
(만약 사용 안 하면 그냥 보류)

Transformer의 디코더 레이어를 정의한 파일입니다.

입력 시퀀스를 보고, 정답 시퀀스를 한 글자씩 생성할 때 사용됩니다.

이 코드에서는 시계열 예측에 직접 쓰이지는 않지만, 추후 생성 문제에 쓸 수 있음.

🔧 MultiHeadAttention.py
'여러 개의 눈'으로 데이터를 보는 부품입니다.

Attention이라는 연산을 여러 번 병렬로 수행해서
다양한 시각에서 데이터를 이해하게 해줍니다.

예를 들면 하나의 헤드는 '단기 변화'를 보고,
다른 헤드는 '장기 추세'를 볼 수 있게 설계된 개념입니다.

🔧 PositionalEncoding.py
“이 값이 언제 들어왔는지”를 알려주는 부품입니다.

Transformer는 순서를 모르기 때문에,
각 시간 위치에 고유한 숫자 패턴을 더해줘서
입력 순서를 모델이 알 수 있게 해줍니다.

시계열 데이터에선 시점 간 차이가 중요하므로 꼭 필요함.

🔧 PositionwiseFeedForward.py
Attention 이후 정보를 더 가공해주는 작은 신경망입니다.

각 위치(time step)별로 독립적으로 적용됩니다.

'데이터 정제소'처럼 생각해도 됨 — 들어온 정보에 비선형 변형을 줘서 더 복잡한 표현을 만들게 함.

🔧 ScaledDotProductAttention.py
두 벡터 간의 “중요도”를 계산해주는 핵심 부품입니다.

이게 바로 “Attention”의 핵심 연산입니다.

입력 벡터들끼리 내적해서 얼마나 관련 있는지(유사도)를 계산하고,
중요한 값에 더 집중해서 처리하게 합니다.

간단히 말하면:
"이 값이 다른 값이랑 얼마나 관련 있어 보이나?"를 수치로 판단해서 반영하는 모듈

